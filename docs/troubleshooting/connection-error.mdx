---
sidebar_position: 0
title: "🚧 服务器连接问题解决指南"
---

我们将帮助你解决各种连接问题，确保系统正常运行。以下是针对不同场景的详细解决方案，包括Ollama本地服务器和Hugging Face等外部服务器的常见连接问题。

## 🌟 Ollama服务器连接问题

### 🚀 Open WebUI无法连接到Ollama

如果你发现Open WebUI无法连接到Ollama，很可能是因为Ollama没有监听允许外部连接的网络接口。让我们按步骤解决这个问题：

1. **配置Ollama网络监听** 🎧：
   将`OLLAMA_HOST`设置为`0.0.0.0`，让Ollama监听所有网络接口。

2. **检查环境变量**：
   确保在你的部署环境中正确配置了`OLLAMA_HOST`变量。

3. **重启Ollama服务**🔄：
   配置修改后需要重启服务才能生效。

💡 完成设置后，请通过访问WebUI界面验证Ollama是否已经可以正常连接。

更多关于Ollama配置的详细信息，请参考[Ollama官方文档](https://github.com/ollama/ollama/blob/main/docs/faq.md#setting-environment-variables-on-linux)。

### 🐳 Docker环境连接错误

如果在Docker环境中无法访问Ollama，可能是因为WebUI容器无法与主机上的Ollama服务器通信。解决方案如下：

1. **修改网络配置** 🛠️：
   在Docker运行命令中添加`--network=host`参数，这样容器就能直接使用主机的网络。

2. **注意端口变化**：
   使用host网络模式时，内部服务端口会从3000变为8080。

**Docker运行命令示例**：
```bash
docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```
🔗 执行上述命令后，你可以通过`http://localhost:8080`访问WebUI。

## 🔒 Hugging Face的SSL连接问题

遇到SSL证书错误？这可能与Hugging Face服务器有关。解决步骤如下：

1. **检查服务状态**：
   首先确认Hugging Face服务器是否存在已知问题或维护。

2. **更换访问节点**：
   如果官方服务器无法访问，可以在Docker命令中切换到镜像节点。

**解决连接问题的Docker命令示例**：
```bash
docker run -d -p 3000:8080 -e HF_ENDPOINT=https://hf-mirror.com/ --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

## 🍏 MacOS系统上的Podman配置

如果你在MacOS上使用Podman，需要特别注意以下配置：

1. **启用主机网络回环**：
   运行命令时添加`--network slirp4netns:allow_host_loopback=true`参数。

2. **配置Ollama访问地址**：
   将`OLLAMA_BASE_URL`设置为`http://host.containers.internal:11434`。

**Podman运行命令示例**：
```bash
podman run -d --network slirp4netns:allow_host_loopback=true -p 3000:8080 -e OLLAMA_BASE_URL=http://host.containers.internal:11434 -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

