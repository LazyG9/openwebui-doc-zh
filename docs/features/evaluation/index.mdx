---
sidebar_position: 6
title: "📝 模型评估"
---

## 为什么要评估模型？

认识一下 **Alex**，一位在中型公司工作的机器学习工程师。Alex 知道市面上有很多 AI 模型——GPT、LLaMA 等等，但哪一个最适合手头的工作呢？这些模型在宣传上都很出色，但 Alex 不能仅仅依赖公开的排行榜。这些模型在不同场景下表现各异，有些模型可能在评估数据集上进行过训练（这有失公平）。而且，这些模型的表达方式有时感觉不够自然。

这正是 Open WebUI 发挥作用的地方。它为 Alex 和团队提供了一种简单的方法，可以根据实际需求评估模型。不需要复杂的数学计算，不需要繁重的工作。只需在与模型互动时给出赞成或反对的评价即可。

### 简要总结

- **评估的重要性**：模型众多，但并非所有模型都适合您的特定需求。公开排行榜并不总是可靠。
- **解决方案**：Open WebUI 提供内置的评估系统。使用点赞/点踩来评价模型回复。
- **后台机制**：评分会更新您的个性化排行榜，被评价的对话快照将用于未来的模型优化！
- **评估方式**：
  - **对战模式**：随机选择模型进行比较。
  - **日常对话**：在日常使用中评价回复。

---

### 为什么公开评估不够用？

- 公开排行榜并不针对**您的**具体使用场景。
- 某些模型在评估数据集上训练过，影响了结果的公平性。
- 一个模型整体表现可能很好，但其表达方式或回复可能不符合您期望的风格。

### 解决方案：Open WebUI 的个性化评估

Open WebUI 内置了评估功能，让您和团队在与模型互动的同时，发现最适合您特定需求的模型。

工作原理很简单！

- **在对话过程中**，喜欢某个回复就点赞，不喜欢就点踩。如果该消息有**关联回复**（比如重新生成的回复或并排比较的模型回复），您就是在为您的**个人排行榜**做贡献。
- **排行榜**可以在管理界面轻松查看，帮助您追踪哪些模型根据团队标准表现最好。

一个特色功能？**每次您评价回复时**，系统会保存该**对话的快照**，这将用于后续改进模型甚至支持未来的模型训练。（注意：此功能仍在开发中！）

---

### 评估 AI 模型的两种方式

Open WebUI 提供了两种直观的 AI 模型评估方法。

### **1. 对战模式**

**对战模式**从可用模型池中随机选择，确保评估公平无偏。这有助于避免手动比较中的一个潜在问题：**生态有效性**——确保您不会有意或无意地偏向某个模型。

使用方法：
- 从对战模式选择器中选择一个模型
- 像平常一样使用它，只是现在您处于"对战模式"

要让您的反馈计入排行榜，您需要**关联回复**。什么是关联回复？就是同一问题产生的其他回复（比如重新生成的回复或多个模型同时生成的回复）。这样，您就是在进行**直接对比**。

- **评分建议**：当您给一个回复点赞时，另一个会自动获得点踩。请谨慎选择，只给您认为确实最好的回复点赞！
- 评价完成后，您可以查看排行榜，了解各个模型的表现。

以下是对战模式界面的预览：

![对战模式示例](/images/evaluation/arena.png)

想要更深入的体验？您还可以设置类似 [**Chatbot Arena**](https://lmarena.ai/) 的多模型对战！

![多模型对战示例](/images/evaluation/arena-many.png)

### **2. 日常对话**

如果您不想使用"对战模式"，也完全可以。您可以正常使用 Open WebUI，像平常一样评价 AI 模型的回复。只需在觉得合适时给模型回复点赞/点踩即可。不过，**如果您希望评价计入排行榜**，您需要**切换不同的模型进行对话**。这样才能产生**关联回复**进行比较——只有不同模型之间的比较才会影响排名。

这是在日常对话中评分的界面：

![常规评分界面](/images/evaluation/normal.png)

这是设置多模型比较的示例：

![多模型比较](/images/evaluation/normal-many.png)

---

## 排行榜

完成评分后，您可以在管理面板中查看**排行榜**。这里展示了模型的表现，使用**等级分系统**（类似象棋排名）进行排名。您可以直观地看到哪些模型在评估中真正脱颖而出。

这是排行榜的显示效果：

![排行榜示例](/images/evaluation/leaderboard.png)

### 按主题重新排名

评价对话时，您可以**添加主题标签**以获得更细致的分析。这在处理不同领域（如**客户服务、创意写作、技术支持**等）时特别有用。

#### 自动标记
Open WebUI 会尝试根据对话内容**自动添加标签**。但是，根据使用的模型不同，自动标记功能可能**有时会出错**或误解对话内容。这种情况下，建议**手动添加标签**以确保准确性。

- **手动标记方法**：评价回复时，您可以根据对话内容添加相应的标签。

请不要忽视这一步！标记功能非常实用，因为它允许您**根据特定主题重新排序模型**。比如，您可以查看哪个模型在处理技术支持问题和一般咨询时分别表现如何。

这是按主题重新排名的效果：

![按主题排序的排行榜](/images/evaluation/leaderboard-reranked.png)

---

### 补充说明：对话快照与模型优化

每次您评价模型回复时，Open WebUI 都会*保存对话快照*。这些快照未来可用于**优化您的模型**——这样您的评价就能持续改进 AI 的表现。

*（更多相关功能正在开发中，敬请期待！）*

---

## 总结

**简而言之**，Open WebUI 的评估系统有两个核心目标：
1. 帮助您**轻松对比模型**
2. 找到最适合您实际需求的模型

本质上，这个系统让 AI 模型评估变得**简单、透明且个性化**。无论是通过对战模式还是日常对话，**您都能完全掌控，找到最适合您特定场景的 AI 模型**！

**请放心**，您的所有数据都安全存储在**您的服务器**上，除非您主动**选择参与社区共享**，否则不会共享任何内容。您的隐私和数据安全始终是我们的首要考虑。
